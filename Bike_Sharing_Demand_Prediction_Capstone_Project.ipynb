{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "MSa1f5Uengrz",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "Yfr_Vlr8HBkt",
        "tEA2Xm5dHt1r",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "578E2V7j08f6",
        "67NQN5KX2AMe",
        "zVGeBEFhpsJ2",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/treasure823/Bike_Sharing_Demand_Prediction_Capstone_Project/blob/main/Bike_Sharing_Demand_Prediction_Capstone_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - Bike Sharing Demand Prediction\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Regression\n",
        "##### **Contribution**    - Individual\n",
        "##### **Team Member 1 -**   Nidhi Pandey\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "HqkIS6nZMYwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Currently Rental bikes are introduced in many urban cities for the enhancement of mobility comfort. It is important to make the rental bike available and accessible to the public at the right time as it lessens the waiting time. Eventually, providing the city with a stable supply of rental bikes becomes a major concern. The crucial part is the prediction of bike count required at each hour for the stable supply of rental bikes.In this study, we train a model to forecast the availability of bikes at any hour of the year based on the weather. The data collection, which included historical bike usage patterns and weather data spanning two years, was collected from the Capital Bikeshare programme in Washington, D.C.\n",
        "The data set is first subjected to exploratory data analysis. We search for missing data values (none were identified) and outliers, then change them as necessary. Additionally, we use correlation analysis to isolate the most crucial and pertinent feature set. Later, we use feature engineering to change a few already-existing columns and eliminate irrelevant ones.\n",
        "Then, we examine a number of well-known individual models, ranging from straightforward ones like Linear Regressor and Regularisation Models (Ridge and Lasso) to more intricate ensemble models, such as Random Forest, Gradient Boost, and Adaboost. A single unified model for working and non-working days, among other choices for model development, were tested. 2. There are two distinct models for workdays and non-workdays. Utilising the provided categorical features, 3. using OneHotEncoding to obtain binary vector representations of the categorical features, and 4. To further improve the predicting skills, we also explored stacking algorithms (Linear Regressor, Random Forest, and Gradient Boost) where the predictions from the level 1 individual models were incorporated as meta-features into a second level model.A portion of the provided training data set (the first 14 days of each month) was used to tweak the hyperparameters using GridSearchCV cross validation using 5 folds. To evaluate the effectiveness of our model, we examined the remaining data (15th to 19th of each month). With train and test scores of 0.36 and 0.427, respectively, we identified the Random Forest Ensemble approach employing a Single Model and Categorical Feature set to be the best option out of all the methods and models. The total train+test observation size is 10871, and the training and testing times for the selected model are pretty reasonable (23 seconds).**"
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -** "
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/treasure823/Book-Recommendation-System_Nidhi-Pandey"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Write Problem Statement Here.**"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Currently Rental bikes are introduced in many urban cities for the enhancement of mobility comfort. It is important to make the rental bike available and accessible to the public at the right time as it lessens the waiting time. Eventually, providing the city with a stable supply of rental bikes becomes a major concern. The crucial part is the prediction of bike count required at each hour for the stable supply of rental bikes.**"
      ],
      "metadata": {
        "id": "8TdKIZ1RNw5g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required. \n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits. \n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule. \n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# lets import all the necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "#this will be used to display all the graphs in the Notebook\n",
        "%matplotlib inline\n",
        "sns.set_style(\"whitegrid\",{'grid.linestyle': '--'})\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "OYypLcpJgxOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "# PAth of dataset \n",
        "data = '/content/drive/MyDrive/Bike_Sharing_Demand_Prediction_Capstone_Project_Nidhi_Pandey/SeoulBikeData.csv'"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "df = pd.read_csv(data, encoding = \"ISO-8859-1\")"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "#Top 5 rows of the data\n",
        "df.head()"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.tail()"
      ],
      "metadata": {
        "id": "VsNBs2LsgaNd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For Checking the shape of the data \n",
        "df.shape\n"
      ],
      "metadata": {
        "id": "-NxZCRohkW7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "df.info() "
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "df.duplicated()"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(df[df.duplicated()])"
      ],
      "metadata": {
        "id": "nXcVtM1iMAAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.notnull().count()"
      ],
      "metadata": {
        "id": "fXDB67_fiAGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "import missingno as msno\n",
        "msno.bar(df, color='orange',sort='ascending', figsize=(10,3), fontsize=15)\n"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset contains weather information (Temperature, Humidity, Windspeed, Visibility, Dewpoint, Solar radiation, Snowfall, Rainfall), the number of bikes rented per hour and date information.\n",
        "\n",
        "Attribute Information:\n",
        "• Date : year-month-day\n",
        "\n",
        "• Rented Bike count - Count of bikes rented at each hour\n",
        "\n",
        "• Hour - Hour of the day\n",
        "\n",
        "• Temperature-Temperature in Celsius\n",
        "\n",
        "• Humidity - %\n",
        "\n",
        "• Windspeed - m/s\n",
        "\n",
        "• Visibility - 10m\n",
        "\n",
        "• Dew point temperature - Celsius\n",
        "\n",
        "• Solar radiation - MJ/m2\n",
        "\n",
        "• Rainfall - mm\n",
        "\n",
        "• Snowfall - cm\n",
        "\n",
        "• Seasons - Winter, Spring, Summer, Autumn\n",
        "\n",
        "• Holiday - Holiday/No holiday\n",
        "\n",
        "• Functional Day - NoFunc(Non Functional Hours), Fun(Functional hours)\n",
        "\n",
        "Data Pipeline:\n",
        "\n",
        "● Exploratory Data Analysis (EDA): In this part we have done some EDA on the features to see the trend.\n",
        "\n",
        "● Data Processing: In this part we went through each attributes and encoded the categorical features.\n",
        "\n",
        "● Model Creation: Finally in this part we created the various models. These various models are being analysed and we tried to study various models so as to get the best performing model for our project.\n"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "df.columns "
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numerical_feature = [ftr for ftr in df.columns if df[ftr].dtype != 'O'] # attributes that are not of Object typer, i.e numerical data\n",
        "categorical_feature = [ftr for ftr in df.columns if df[ftr].dtype == 'O'] \n",
        "target = ['Rented Bike Count']\n",
        "numerical_feature.remove(target[0])"
      ],
      "metadata": {
        "id": "2BQFid08Ml9G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "df.describe().transpose()"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe(include='O')"
      ],
      "metadata": {
        "id": "p8t_GJ_V_s3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description "
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset contains weather information (Temperature, Humidity, Windspeed, Visibility, Dewpoint, Solar radiation, Snowfall, Rainfall), the number of bikes rented per hour and date information.\n",
        "\n",
        "Attribute Information:\n",
        "\n",
        "Date : year-month-day\n",
        "\n",
        "Rented Bike count - Count of bikes rented at each hour\n",
        "\n",
        "Hour - Hour of the day\n",
        "\n",
        "Temperature-Temperature in Celsius\n",
        "\n",
        "Humidity - %\n",
        "\n",
        "Windspeed - m/s\n",
        "\n",
        "Visibility - 10m\n",
        "\n",
        "Dew point temperature - Celsius\n",
        "\n",
        "Solar radiation - MJ/m2\n",
        "\n",
        "Rainfall - mm\n",
        "\n",
        "Snowfall - cm\n",
        "\n",
        "Seasons - Winter, Spring, Summer, Autumn\n",
        "\n",
        "Holiday - Holiday/No holiday\n",
        "\n",
        "Functional Day - NoFunc(Non Functional Hours), Fun(Functional hours)\n",
        "\n"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "df.nunique()\n",
        " "
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupby('Date').agg({'Hour':'count'}).Hour.unique()"
      ],
      "metadata": {
        "id": "2JYeEKqt-rCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**There are 24 logs of bike rental data provided in the dataset**"
      ],
      "metadata": {
        "id": "ZkpAHdiBBe5A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.Hour.unique()"
      ],
      "metadata": {
        "id": "t3lIXAzo6fXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in df.columns.tolist():\n",
        "  print(\"No. of unique values in \",i,\"is\",df[i].nunique())\n",
        "     "
      ],
      "metadata": {
        "id": "O5jK8qTJrVrH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in categorical_feature :\n",
        "  print(col , df[col].unique() , '/n')"
      ],
      "metadata": {
        "id": "010UPsRI_C74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "# Convert of date columns to dateformat\n",
        "df['Date']= pd.to_datetime(df['Date'])"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets extracting days, months, days of week and weekdays / weekend from date column from the dataset\n",
        "df['Date']=pd.to_datetime(df['Date'])\n",
        "df['month'] = df['Date'].apply(lambda x : x.month)\n",
        "df['day_of_week'] = df['Date'].dt.day_name()\n",
        "df['weekdays_weekend']=df['day_of_week'].apply(lambda x : 1 if x=='Saturday' or x=='Sunday' else 0 )\n",
        "df=df.drop(columns=['Date','day_of_week'],axis=1)"
      ],
      "metadata": {
        "id": "8sfnj1FF_dcL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_1 = df.iloc[:6,:5]"
      ],
      "metadata": {
        "id": "1-gDSxqHAHiT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_2 = df.iloc[2:4, :7]"
      ],
      "metadata": {
        "id": "6rAVDwxBGXwy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.concat = ([df_1, df_2])"
      ],
      "metadata": {
        "id": "rxdgzT1UGQU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data wrangling is the process of converting raw data into a usable form. It may also be called data munging or data remediation. You'll typically go through the data wrangling process prior to conducting any data analysis in order to ensure your data is reliable and complete.\n",
        "I have converted  date columns to dateformat and concated two columns.**"
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code for the Rented Bike Count \n",
        "sns.distplot(df['Rented Bike Count'])\n"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**distplot() function is used to plot the distplot. The distplot represents the univariate distribution of data i.e. data distribution of a variable against the density distribution. The seaborn. distplot() function accepts the data variable as an argument and returns the plot with the density distribution.**"
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have observed the density for rented bike count"
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No"
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "# Square root transformation \n",
        "plt.figure(figsize = (8,9))\n",
        "sns.distplot(np.sqrt(df['Rented Bike Count' ]))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**distplot() function is used to plot the distplot. The distplot represents the univariate distribution of data i.e. data distribution of a variable against the density distribution. The seaborn. distplot() function accepts the data variable as an argument and returns the plot with the density distribution.**"
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We done the squAre root transformation in the above distplot .We observe the density of the rented bike count .**"
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No"
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "df= df.iloc [:, ]\n",
        "df.head(5)"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.tail(5)"
      ],
      "metadata": {
        "id": "6c-sREVYSQRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.lineplot(data = df )"
      ],
      "metadata": {
        "id": "9qpgoHP0SYar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**To determine the relationship between two datasets, line graphs are utilised. An assortment of values make up a dataset. The x and y axes are used to plot each dataset**"
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have observe the data the  straight pink line for rented bike dotted yellow line for hour."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No"
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "# Extraction of Categorical columns\n",
        "categorical_feature = df.select_dtypes(include='object')\n",
        "\n"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_feature"
      ],
      "metadata": {
        "id": "c6CEVdQUg9yU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Potting Box plot for visualisation so as to get information  from plot \n",
        "for col in categorical_feature :\n",
        "  plt.figure(figsize = (5,6))\n",
        "  sns.boxplot(x= df[col], y = df[\"Hour\"])\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "pRu_qdFzhJTf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Compared to matplotlib, Seaborn offers considerably prettier plots right out of the box. Boxplots help us visualise our data and comprehend its range and distribution. But they are a great tool for spotting outliers in your data.**"
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can say that maximum no.of bikes are rented during the evening hours"
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No "
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "# Extraction of the numeric features\n",
        "numeric_feature = df.select_dtypes(exclude = 'object')\n"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Numeric columns info\n",
        "numeric_feature.info()"
      ],
      "metadata": {
        "id": "ev8gCB6qa3pI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Describe the numeric feature\n",
        "numeric_feature.describe().transpose()\n"
      ],
      "metadata": {
        "id": "69qq8cSIbWRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot histogtram for dataset\n",
        "for col in numeric_feature[:]:\n",
        "  sns.histplot(df[col])\n",
        "  plt.axvline(df[col].mean(), color='maroon', linestyle='dashed', linewidth=2)\n",
        "  plt.axvline(df[col].median(), color='cyan', linestyle='dashed', linewidth=2)   \n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "pv6GeYlicbxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A histogram is a graph showing frequency distributions. It is a graph showing the number of observations within each given interval.**"
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**With the help of this graph we can compare all the values** "
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No "
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "# Plotting of the Regression plot for each columns of dataset v/s Temperature(°C) columns\n",
        "for col in numeric_feature[:]:\n",
        "  if col == 'Temperature(°C)':\n",
        "    pass\n",
        "  else:\n",
        "    sns.regplot(x=df[col],y=df[\"Temperature(°C)\"],line_kws={\"color\": \"red\"})\n",
        "  \n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**compared to matplotlib, Seaborn offers considerably prettier plots right out of the box. Boxplots help us visualise our data and comprehend its range and distribution. But they are a great tool for spotting outliers in your data**"
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**THe temperature decreases in winter season **"
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No "
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "# for checking the counts of functioning days\n",
        "df['Functioning Day'].value_counts()"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cat plot for getting more information\n",
        "sns.catplot(x='Seasons',y='Rented Bike Count',data=df)"
      ],
      "metadata": {
        "id": "I9XmXPP_ke1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Categorical plots are created using the Seaborn. catplot() function. This function provides users with access to a number of axes-level functions that show the relationship between numerical data and one or more category variables using one of various visual representations**"
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**It can be clearly seen that there is less demand of rented bike during winter season**"
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **The business has negative impact only in winter season as the bikes demand is deacreasing in winter and it is clear from the data.**"
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "# Cat plotting so as to get more information\n",
        "\n",
        "feature_list=[\"Hour\",\"Holiday\",'Rainfall(mm)','Snowfall (cm)','Temperature(°C)']\n",
        "for feature in feature_list:\n",
        "  plt.figure(figsize=(5,8),dpi=200)\n",
        "  sns.catplot(x=feature,y='Rented Bike Count',data=df)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Categorical plots are created using the Seaborn. catplot() function. This function provides users with access to a number of axes-level functions that show the relationship between numerical data and one or more category variables using one of various visual representations**\n",
        "\n"
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inference drawn from the Cat Plot:\n",
        "\n",
        "**From hour v/s rented bike**\n",
        "* **It is clearly visible that there is high demand of Rented bike during the office hours.**\n",
        "\n",
        "**From working-nonworking v/s rented bike**\n",
        "\n",
        "* **It can be seen from 2nd plot that working days has comparatively high demand of rented bike as compared to during the non working days.**\n",
        "\n",
        "**From Rainfall v/s rented bike**\n",
        "\n",
        "* **It can be seen that if Rainfall increases demand of Rented Bike Decreases.**\n",
        "\n",
        "**From Snowfall v/s rented bike**\n",
        "\n",
        "* **Also we can see that if Snowfall increases demand of Rented Bike Decreases.**"
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **The business has negative impact only in winter season as the bikes demand is deacreasing in winter and it is clear from the data.**\n"
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas._libs.tslibs.offsets import Hour\n",
        "# Chart - 9 visualization code\n",
        "#Plotting line graph\n",
        "#Grouping by hours to get the averarge price Bike rented and the percentage change \n",
        "avg_rent_hrs = df.groupby('Hour')['Rented Bike Count'].mean()\n"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the average rent over time \n",
        "plt.figure(figsize = (10,4))\n",
        "a= avg_rent_hrs.plot(legend = True ,marker ='o', title=\"Averages Bike Rented Per Hr\")\n",
        "a.set_xticks(range(len(avg_rent_hrs)));\n",
        "a.set_xticklabels(avg_rent_hrs.index.tolist(),rotation = 85);\n"
      ],
      "metadata": {
        "id": "bv6nGHW-z6fZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**To determine the relationship between two datasets, line graphs are utilised. An assortment of values make up a dataset. The x and y axes are used to plot each datasetAnswer**"
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inferences which can be drawn:**\n",
        "\n",
        " >* **There is high rise in Rented Bikes from 8:00 a.m to 9:00 p.m which means people prefer renting the bikes during rush hour.**\n",
        "\n",
        ">* **We can clearly see that there is a rise in demand from 8 a.m to 6:00 p.m so we can say that during office opening and closing time there is a high demand.**"
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NO the above data doesn't provide us any such information.**"
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code\n",
        "plt.figure(figsize =(7,5) ,dpi = 100)\n",
        "sns.boxplot( x= 'Seasons' ,y ='Rented Bike Count' ,data = df)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Compared to matplotlib, Seaborn offers considerably prettier plots right out of the box. Boxplots help us visualise our data and comprehend its range and distribution. But they are a great tool for spotting outliers in your data.**"
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 11 visualization code\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "plt.figure(figsize=(20, 15))\n",
        "plt.suptitle(\"Count Plot\", fontsize=18, y=0.95)\n",
        "\n",
        "for n, ticker in enumerate(categorical_feature):\n",
        "  # add a new subplot iteratively\n",
        "\n",
        "  # filter df and plot ticker on the new subplot axis\n",
        "  ax = plt.subplot(4,3, n+1)\n",
        "  plt.subplots_adjust(hspace=0.5, wspace=0.3)\n",
        "  sns.countplot(x=df[ticker])   \n",
        "  ax.set_title(ticker.upper())\n",
        "  ax.set_xlabel(\"\")\n"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Instead of using a quantitative variable, a count plot can be thought of as a histogram across a categorical one. You can compare counts across nested variables because the fundamental API and settings are the same as those for barplot().**"
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The bike demand decreases during winter season, on holidays**"
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No"
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12\n",
        "## **Trend of Bike Sharing on an average day** "
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 12 visualization code\n",
        "plt.figure(figsize = (12,8))\n",
        "sns.lineplot(x = 'Hour', y= 'Rented Bike Count', data = df)\n",
        "plt.title(\"Average Bike Sharing Demand\")                                                                                                                                                                                                                                                #mahinisawesomemate\n",
        "a = plt.xticks(ticks = np.arange(0,24,1))"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (10,8))\n",
        "sns.lineplot(x = 'Hour', y= 'Rented Bike Count',hue ='Seasons' , data = df)\n",
        "plt.title(\"Average Bike Sharing Demand on Different  seasons\")\n",
        "a = plt.xticks(ticks = np.arange(0,24,1))"
      ],
      "metadata": {
        "id": "C6YgfSEXELaM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (16,8))\n",
        "sns.lineplot(x = 'Hour', y= 'Rented Bike Count',hue ='Holiday' , data = df)\n",
        "plt.title(\"Bike Sharing Demand: Holidays vs Working Days\")\n",
        "a = plt.xticks(ticks = np.arange(0,24,1))"
      ],
      "metadata": {
        "id": "oiHAOmQfcZe_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "   **To determine the relationship between two datasets, line graphs are utilised. An assortment of values make up a dataset. The x and y axes are used to plot each dataset**"
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        ">* **Winter season recieves the least bike sharing demand of all the seasons  while Summer is observed to be the season of maximum bike sharing demand**\n",
        ">* **The peaks and lows are similar in all the seasons suggesting that the rental routines of the people don't change but the amount of demand certainly does**"
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No"
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 13 visualization \n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "plt.figure(figsize=(20, 15))\n",
        "plt.suptitle(\"Regression Plot\", fontsize=12, y=0.95)\n",
        "\n",
        "for n, ticker in enumerate(numerical_feature):\n",
        "  # add a new subplot iteratively\n",
        "\n",
        "  # filter df and plot ticker on the new subplot axis\n",
        "  ax = plt.subplot(4,3, n+1)\n",
        "  plt.subplots_adjust(hspace=0.5, wspace=0.3)\n",
        "  sns.regplot(x=df[ticker],y=df['Rented Bike Count'],line_kws={\"color\": \"red\"})   \n",
        "  ax.set_title(ticker.upper())\n",
        "  ax.set_xlabel(\"\")"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The main purpose of the regression plots in the Python Seaborn library is to provide a visual aid for emphasising trends in a dataset during exploratory data analysis. Regression plots, as the name implies, produce a regression line between two parameters and aid in visualising their linear correlations.**"
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Feature Extraction\n",
        "---\n",
        "**In order to get deeper insights, we'll be extracting features from the date column. The relevant features extracted will be:**\n",
        "1.   Weekend: Boolean variable tells if the day falls on a weekend\n",
        "2.   day of the week\n",
        "3.   Month\n",
        "**Features extracted from hour column will be:**\n",
        "4. Day Phase: Morning, Afternoon,Evening and Night"
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No "
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization\n",
        "# Now plotting the correlation with the help of heatmap\n",
        "plt.figure(figsize=(15,10))\n",
        "sns.heatmap(df.corr(),cmap='PiYG',annot=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# find and remove correlated features\n",
        "\n",
        "def correlation(dataset, threshold):\n",
        "    col_corr = set()                                           # Set of all the names of correlated features\n",
        "    corr_matrix = dataset.corr()\n",
        "    for i in range(len(corr_matrix.columns)):\n",
        "        for j in range(i):\n",
        "            if abs(corr_matrix.iloc[i, j]) > threshold:        # we are interested in absolute coeff value\n",
        "                colname = corr_matrix.columns[i]               # getting the name of column\n",
        "                col_corr.add(colname)\n",
        "    return col_corr\n",
        "     \n"
      ],
      "metadata": {
        "id": "ZOc3_dccosEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# checking the highly correlated features\n",
        "correlation(df, 0.7)          # setting threshold of 0.7"
      ],
      "metadata": {
        "id": "xf7C4P7ooxSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Multicollinearity\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "def calc_vif(X):\n",
        " \n",
        "   # Calculate VIF\n",
        "   vif = pd.DataFrame()\n",
        "   vif[\"variables\"] = X.columns\n",
        "   vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
        " \n",
        "   return(vif)\n",
        "\n"
      ],
      "metadata": {
        "id": "CzfEe6kxHg0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calc_vif(df[[i for i in df.describe().columns if i not in ['Temperature(°C)','Dew point temperature(°C)'] ]])"
      ],
      "metadata": {
        "id": "g7QSKEIab_3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The strength and direction of the linear links between two sets of variables are evaluated using correlation coefficients. Calculating the pairwise correlation coefficients between two or more (numeric) variables is made simple by using a correlation matrix.**\n",
        "* **The correlation coefficient is a numerical measure of the strength and direction of a linear relationship between two variables. In other words, it measures the extent to which changes in one variable are associated with changes in the other variable. The correlation coefficient ranges from -1 to 1, with -1 indicating a perfect negative correlation, 1 indicating a perfect positive correlation, and 0 indicating no correlation.**\n",
        "\n",
        "* **The correlation coefficient is an important tool in data analysis and machine learning, as it can help to identify relationships between variables and can be used in feature selection techniques to remove highly correlated features, which can reduce overfitting and improve the performance of the model.**"
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "  * **WE got toknow that \"dew_point_temperature\" and temperature have a correlation coefficient of 0.91 and dew_point_temperature is less correlated to our target variable hence dropping dew_point_temperature.**"
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot "
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "plt.figure(figsize=(20, 15))\n",
        "plt.suptitle(\"Distribution Plot\", fontsize=18, y=0.95)\n",
        "\n",
        "for n, ticker in enumerate(numerical_feature + target):\n",
        "  # add a new subplot iteratively\n",
        "  ax = plt.subplot(4,3, n + 1)\n",
        "  plt.subplots_adjust(hspace=0.5, wspace=0.3)\n",
        "  # filter df and plot ticker on the new subplot axis\n",
        "  sns.distplot(df[ticker])\n",
        "  plt.axvline(df[ticker].mean(), color='red', linestyle='dashed', linewidth=2)\n",
        "  plt.axvline(df[ticker].median(), color='cyan', linestyle='dashed', linewidth=2)   \n",
        "  ax.set_title(ticker.upper())\n",
        "  ax.set_xlabel(\"\")"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pair plots are used to determine the most distinct clusters or the best combination of features to describe a connection between two variables. By creating some straightforward linear separations or basic lines in our data set, it also helps to create some straightforward classification models.**"
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "> * **Other than 'Hour', all numerical features exhibit some resemblance to a normal distribution.**       \n",
        "> * **The distribution for numerical features like Rented Bike Count,  Solar Radiation and Visibility appear highly skewed, indicating the presence of large outliers.**\n",
        "---"
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) Checking if the data is normally distributed or not.\n",
        "2) checking  "
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A hypothesis can be defined as a proposed explanation for a phenomenon. It is not the absolute truth but a provisional working assumption. It is called a hypothesis because it is not known whether or not it is true.\n",
        "\n",
        "There are two types of hypothesis.\n",
        "\n",
        "Null hypothesis is a statement that the value of a population parameter like mean or proportion is equal to some claimed value. We can either reject the null hypothesis or fail to reject the null hypothesis.\n",
        "\n",
        "The alternative hypothesis is the statement that the statistics has a value that somehow differes from the null hypothesis.\n",
        "\n",
        "A hypothesis test is a standard procedure for testing a claim about a property of a population.\n",
        "\n",
        "\n",
        "\n",
        "Null Hypothesis (H0): mean>= 7LPA\n",
        "\n",
        "Alternative Hypothesis (H1): mean< 7LPA"
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "import numpy as np\n",
        "import seaborn as sns \n",
        "from scipy.stats import *\n",
        "import math "
      ],
      "metadata": {
        "id": "Rzshezl8i4AW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#helper functions \n",
        "class findz:\n",
        "  def proportion(self,sample,hyp,size):\n",
        "    return (sample - hyp)/math.sqrt(hyp*(1-hyp)/size)\n",
        "  def mean(self,hyp,sample,size,std):\n",
        "    return (sample - hyp)*math.sqrt(size)/std\n",
        "  def varience(self,hyp,sample,size):\n",
        "    return (size-1)*sample/hyp"
      ],
      "metadata": {
        "id": "t-RAerqkkEK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "variance = lambda x : sum([(i - np.mean(x))**2 for i in x])/(len(x)-1)"
      ],
      "metadata": {
        "id": "eUC6_DAwkMSX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "findz = findz()"
      ],
      "metadata": {
        "id": "2B1sLCkskR1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def conclusion(p):\n",
        "  significance_level = 0.05\n",
        "  if p>significance_level:\n",
        "    return f\"Failed to reject the Null Hypothesis for p = {p}.\"\n",
        "  else:\n",
        "    return f\"Null Hypothesis rejected Successfully for p = {p}\""
      ],
      "metadata": {
        "id": "sQYzPF96kW9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zcdf = lambda x: norm(0,1).cdf(x)"
      ],
      "metadata": {
        "id": "nYsyd-7Bkd3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def p_value(z,tailed):\n",
        "  z=zcdf(z)\n",
        "  if tailed=='l':\n",
        "    return z\n",
        "  elif tailed == 'r':\n",
        "    return 1-z\n",
        "  elif tailed == 'd':\n",
        "    if z>0.5:\n",
        "      return 2*(1-z)\n",
        "    else:\n",
        "      return 2*z\n",
        "  else:\n",
        "    return np.nan"
      ],
      "metadata": {
        "id": "V4rX1rT6knzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hypothesis_mean = 100\n",
        "sample_mean = 105\n",
        "std = 20\n",
        "size = 50"
      ],
      "metadata": {
        "id": "OhOWZvhmk7ZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "z = findz.mean(hypothesis_mean,sample_mean,size,std)"
      ],
      "metadata": {
        "id": "bcuv8zuYk_WR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p = p_value(z,'r') #right tailed test"
      ],
      "metadata": {
        "id": "ISsAo0Q7lF22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "#creating list of matrix to store the evaluation matrix of all model\n",
        "mean_sq_error=[]\n",
        "root_mean_sq_error=[]\n",
        "r2_list=[]\n",
        "adj_r2_list=[]"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We have done right tailed test .**"
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A right tailed test (sometimes called an upper test) is where your hypothesis statement contains a greater than (>) symbol.**"
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.heatmap(df.isnull(), cbar=True, yticklabels=False)\n",
        "plt.xlabel(\"column_name\", size=10, weight=\"bold\")\n",
        "plt.title(\"missing values in column\",fontweight=\"bold\",size=17)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.loc[df.Hour.notnull()]"
      ],
      "metadata": {
        "id": "zSxAJ7Ir0mjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Imputation is a technique used for replacing the missing data with some substitute value to retain most of the data/information of the dataset.**"
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "# figsize\n",
        "plt.figure(figsize=(15,5))\n",
        "# title\n",
        "plt.suptitle('Outlier Analysis of Numerical Feature', fontsize=20, fontweight='bold', y=1.02)\n",
        "\n",
        "for i,col in enumerate(numerical_feature):\n",
        "  plt.subplot(2, 8, i+1)            # subplot of 2 rows and 3 columns\n",
        "\n",
        "  # countplot\n",
        "  sns.boxplot(df[col])\n",
        "  # x-axis label\n",
        "  plt.xlabel(col)\n",
        "  plt.tight_layout()"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we are going to replace the datapoints with upper and lower bound of all the outliers\n",
        "\n",
        "def clip_outliers(df):\n",
        "    for col in df[numerical_feature]:\n",
        "        # using IQR method to define range of upper and lower limit.\n",
        "        q1 = df[col].quantile(0.25)\n",
        "        q3 = df[col].quantile(0.75)\n",
        "        iqr = q3 - q1\n",
        "        lower_bound = q1 - 1.5 * iqr\n",
        "        upper_bound = q3 + 1.5 * iqr\n",
        "        \n",
        "        # replacing the outliers with upper and lower bound\n",
        "        df[col] = df[col].clip(lower_bound, upper_bound)\n",
        "    return df"
      ],
      "metadata": {
        "id": "KeBbpzuYi4j5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# using the function to treat outliers \n",
        "df = clip_outliers(df)"
      ],
      "metadata": {
        "id": "ViGagniFjNcf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking the boxplot after outlier treatment\n",
        "\n",
        "# figsize\n",
        "plt.figure(figsize=(15,5))\n",
        "# title\n",
        "plt.suptitle('Outlier Analysis of Numerical Feature', fontsize=20, fontweight='bold', y=1.02)\n",
        "\n",
        "for i,col in enumerate(numerical_feature):\n",
        "  plt.subplot(2, 8, i+1)            # subplot of 2 rows and 4 columns\n",
        "\n",
        "  # countplot\n",
        "  sns.boxplot(df[col])\n",
        "  # x-axis label\n",
        "  plt.xlabel(col)\n",
        "  plt.tight_layout()\n",
        "     "
      ],
      "metadata": {
        "id": "PMp6fWkmjhFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking for distribution after treating outliers.\n",
        "\n",
        "# figsize\n",
        "plt.figure(figsize=(15,6))\n",
        "# title\n",
        "plt.suptitle('Data Distibution of Numerical Features', fontsize=20, fontweight='bold', y=1.02)\n",
        "\n",
        "for i,col in enumerate(numerical_feature):\n",
        "  plt.subplot(2, 8, i+1)                       # subplots 2 rows, 2 columns\n",
        "\n",
        "  # dist plots\n",
        "  sns.distplot(df[col])  \n",
        "  # x-axis label\n",
        "  plt.xlabel(col)\n",
        "  plt.tight_layout()"
      ],
      "metadata": {
        "id": "YidpkFnDkXrg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clipping Method: In this method, we set a cap on our outliers data, which means that if a value is higher than or lower than a certain threshold, all values will be considered outliers. This method replaces values that fall outside of a specified range with either the minimum or maximum value within that range."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "#Creating Dummy Variables for Categorical Columns\n",
        "dummy_categorical_feature = pd.get_dummies(categorical_feature , drop_first =True)\n",
        "\n"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_categorical_feature "
      ],
      "metadata": {
        "id": "A3Hr8r2U31xf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#concating numeric columns and dummy columns and creating final df \n",
        "final_df= pd.concat([dummy_categorical_feature , numeric_feature ] , axis = 1)\n"
      ],
      "metadata": {
        "id": "E24MKIYZF5gj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# showing the head of the final df \n",
        "final_df.head()"
      ],
      "metadata": {
        "id": "X4f3w32-GyVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# describing final df \n",
        "final_df.describe().transpose()"
      ],
      "metadata": {
        "id": "CAyQC-3SHI20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot a boxplot for the label by each categorical feature  \n",
        "\n",
        "for col in categorical_feature:\n",
        "    fig = plt.figure(figsize=(7, 5))\n",
        "    ax = fig.gca()\n",
        "    df.boxplot(column = 'Hour', by = col, ax = ax)\n",
        "    ax.set_title('Label by ' + col)\n",
        "    ax.set_ylabel(\"Temperature(°C)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "eH8GXC5DJ_L6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "sklearn.set_config(transform_output=\"pandas\")"
      ],
      "metadata": {
        "id": "6YQyaBBRc-Kl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install category_encoders"
      ],
      "metadata": {
        "id": "ffWcxHYhdAbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import category_encoders as ce\n",
        "encoder = ce.BackwardDifferenceEncoder(cols=[...])"
      ],
      "metadata": {
        "id": "Ml0yoM1UdpXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = ce.OneHotEncoder(cols=[...])"
      ],
      "metadata": {
        "id": "dzTcugecd8qd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A set of scikit-learn-style transformers for encoding categorical variables into numeric with different techniques. While ordinal, one-hot, and hashing encoders have similar equivalents in the existing scikit-learn version, the transformers in this library all share a few useful properties:\n",
        "\n",
        "* **First-class support for pandas dataframes as an input (and optionally as output)**\n",
        "\n",
        "* **Can explicitly configure which columns in the data are encoded by name or index, or infer non-numeric columns regardless of input type**\n",
        "\n",
        "* **Can drop any columns with very low variance based on training set optionally**\n",
        "\n",
        "* **Portability: train a transformer on data, pickle it, reuse it later and get the same thing out.**\n",
        "\n"
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing \n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction\n",
        "!pip install contractions"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import library\n",
        "import contractions\n",
        "# contracted text\n",
        "text = '''I'll be there within 5 min. Shouldn't you be there too?\n",
        "          I'd love to see u there my dear. It's awesome to meet new friends.\n",
        "          We've been waiting for this day for so long.'''\n",
        " \n",
        "# creating an empty list\n",
        "expanded_words = []   \n",
        "for word in text.split():\n",
        "  # using contractions.fix to expand the shortened words\n",
        "  expanded_words.append(contractions.fix(word))  \n",
        "   \n",
        "expanded_text = ' '.join(expanded_words)\n",
        "print('Original text: ' + text)\n",
        "print('Expanded_text: ' + expanded_text)"
      ],
      "metadata": {
        "id": "KcWU0wpL_PMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing\n",
        "string = \"DEMAND OF BIKE DECREASES IN WINTER!\"\n",
        "print(string.lower())\n",
        "\n",
        "# string with numbers\n",
        "# all alphabets should be lowercase\n",
        "string = \"D3MAND 0F B!KE D3CREASES IN W!NT3R!\"\n",
        "print(string.lower())"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations\n",
        "# initializing string\n",
        "test_str = \"In, winter bike : demands are ! Low ;\"\n",
        " \n",
        "# printing original string\n",
        "print(\"The original string is : \" + test_str)\n",
        " \n",
        "# initializing punctuations string\n",
        "punc = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
        " \n",
        "# Removing punctuations in string\n",
        "# Using loop + punctuation string\n",
        "for ele in test_str:\n",
        "    if ele in punc:\n",
        "        test_str = test_str.replace(ele, \"\")\n",
        " \n",
        "# printing result\n",
        "print(\"The string after punctuation filter : \" + test_str)"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs \n",
        "import re\n",
        "t =\"This is a text with a URL https://www.java2blog.com/ to remove.\"\n",
        "s1 = re.sub('http://\\S+|https://\\S+', '', t)\n",
        "print(s1)  "
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords\n",
        "no_stpwords_string=\"\"\n",
        "for i in lst_string:\n",
        "    if not i in stop_words:\n",
        "        no_stpwords_string += i+' '\n",
        "        # removing last space\n",
        "no_stpwords_string = no_stpwords_string[:-1]"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White spaces\n",
        "no_wspace_string = no_punc_string.strip()\n",
        "no_wspace_string"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrase Text\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "text = \"\"\" Currently Rental bikes are introduced in many urban cities for the enhancement of mobility comfort. It is important to make the rental bike available and accessible to the public at the right time as it lessens the waiting time. Eventually, providing the city with a stable supply of rental bikes becomes a major concern.  \"\"\"\n",
        "data = text.split('.')\n",
        "for i in data:\n",
        "    print (i)\n"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)\n",
        "# import regex\n",
        "import re\n",
        " \n",
        "# download stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        " \n",
        "# import nltk for stopwords\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        " \n",
        " \n",
        "# input string\n",
        "string = \"       Python 3.0, released in 2008, was a major revision of the language that is not completely backward compatible and much Python 2 code does not run unmodified on Python 3. With Python 2's end-of-life, only Python 3.6.x[30] and later are supported, with older versions still supporting e.g. Windows 7 (and old installers not restricted to 64-bit Windows).\"\n",
        " \n",
        "# convert to lower case\n",
        "lower_string = string.lower()\n",
        " \n",
        "# remove numbers\n",
        "no_number_string = re.sub(r'\\d+','',lower_string)\n",
        " \n",
        "# remove all punctuation except words and space\n",
        "no_punc_string = re.sub(r'[^\\w\\s]','', no_number_string)\n",
        " \n",
        "# remove white spaces\n",
        "no_wspace_string = no_punc_string.strip()\n",
        "no_wspace_string\n",
        " \n",
        "# convert string to list of words\n",
        "lst_string = [no_wspace_string][0].split()\n",
        "print(lst_string)\n",
        " \n",
        "# remove stopwords\n",
        "no_stpwords_string=\"\"\n",
        "for i in lst_string:\n",
        "    if not i in stop_words:\n",
        "        no_stpwords_string += i+' '\n",
        "         \n",
        "# removing last space\n",
        "no_stpwords_string = no_stpwords_string[:-1]\n",
        " \n",
        "# output\n",
        "print(no_stpwords_string)"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lemmatization is a text pre-processing technique used in natural language processing (NLP) models to break a word down to its root meaning to identify similarities. For example, a lemmatization algorithm would reduce the word better to its root word, or lemme, good. Because this technique gives more efficient result.**"
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pos tagging\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "sentence = \"Demands of bikes decreased in winter.\\\n",
        " Demands of bike increases in morning hour\"\n",
        "for token in nlp(sentence):\n",
        "    print(f'{token.text:{10}} {token.tag_:>{10}}\\t{spacy.explain(token.tag_):<{50}} {token.pos_:>{5}}')"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pprint \n",
        "pp = pprint.PrettyPrinter(indent=4)\n",
        "cv = CountVectorizer()\n",
        "sentence = ['demands of bikes decreased in winter' , 'demands of bike increases in morning hour']\n",
        "bow_rep = cv.fit_transform(sentence)\n",
        "\n",
        "pp.pprint(cv.vocabulary_)\n",
        "\n",
        "print(\"BoW representaion for {}:\".format(sentence[0]),bow_rep[0].toarray())\n",
        "print(\"BoW representaion for {}:\".format(sentence[1]),bow_rep[1].toarray())\n"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def OHE(text):\n",
        "    tokens = set(text.lower().split())\n",
        "    length = len(tokens)\n",
        "    index_map = {x:index for x,index in zip(tokens,range(length))}\n",
        "    ohe_matrix = [] \n",
        "    \n",
        "    for token in tokens:\n",
        "                ohe = np.zeros(length)\n",
        "                ohe[index_map[token]] = 1\n",
        "                print(token,ohe)\n",
        "                ohe_matrix.append(ohe)\n",
        "OHE('demands of bikes decreased in winter')"
      ],
      "metadata": {
        "id": "ECtvQHnhOLRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Word vectorization is a methodology in NLP to map words or phrases from vocabulary to a corresponding vector of real numbers which used to find word predictions, word similarities/semantics.**"
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "plt.figure(figsize=(25, 10))\n",
        "plt.suptitle('BOX PLOT of Numerical Feature', fontsize=16, y=0.95)\n",
        "\n",
        "#Looping through numerical features and looking for outliers using BOX PLOT\n",
        "for n, ticker in enumerate(numerical_feature + target):\n",
        "  ax = plt.subplot(4,3, n+1)\n",
        "  plt.subplots_adjust(hspace=0.5, wspace=0.3)\n",
        "  sns.boxplot(x = df[ticker])\n",
        "  ax.set_title(ticker.upper())\n",
        "  ax.set_xlabel(\"\")"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting\n",
        "engineered_df = df.copy()\n",
        "for ftr in categorical_feature:\n",
        "  engineered_df = pd.concat([pd.get_dummies(engineered_df[ftr],prefix = ftr, drop_first=True), engineered_df.drop(ftr, axis = 1)], axis =1)"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "engineered_df.info()"
      ],
      "metadata": {
        "id": "Wmd4EH-e95hH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**No i don't think so that the data need to be transformed because it already contains all the necessary information to implement on the dataset/project**"
      ],
      "metadata": {
        "id": "ceDqilkuVGhQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler"
      ],
      "metadata": {
        "id": "e0pMbviqX1n7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "#function to scale\n",
        "def do_scale(X_train, X_test, scaling_type = StandardScaler):\n",
        "  scaler = scaling_type()\n",
        "  scaler.fit(X_train)\n",
        "  X_train_scaled = pd.DataFrame(scaler.transform(X_train), columns = X_train.columns)\n",
        "  X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns = X_test.columns)                                                                                                                                                                                                                                                #mahinisawesomemate\n",
        "  return X_train_scaled, X_test_scaled"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "pip install shap"
      ],
      "metadata": {
        "id": "GQC0vHQ_7yhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shap"
      ],
      "metadata": {
        "id": "oLYFOpI38TQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Dictionary that will store metrics for evaluated models\n",
        "#This will be converted DataFrame using display report function\n",
        "report = {\n",
        "    'model_type':[],\n",
        "    'model_name':[],                                                                                                                                                                                                                                                #mahinisawesomemate\n",
        "    'rmse':[],\n",
        "    'mae':[],\n",
        "    'R2':[],\n",
        "    'adjusted R2':[]\n",
        "\n",
        "}"
      ],
      "metadata": {
        "id": "9VUupyZ68Yqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"StandardScaler\" \n",
        "\n",
        "**StandardScaler removes the mean and scales each feature/variable to unit variance. **"
      ],
      "metadata": {
        "id": "ik1Xx8rw4pKU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**No i don't think so that that dimensionality reduction is needed.**"
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# splitting data into train and test set\n",
        "X,y = engineered_df.drop('Rented Bike Count', axis=1), engineered_df['Rented Bike Count']                                                                                                                                                                                                                                                #mahinisawesomemate\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size =0.2,  random_state=5)"
      ],
      "metadata": {
        "id": "TgizWSMQ7BDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "# Train - Spliting  data\n",
        "X=final_df.drop(['Rented Bike Count'],axis=1)"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "id": "vks-Q7knKdsA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y=np.sqrt(final_df['Rented Bike Count'])"
      ],
      "metadata": {
        "id": "vFNMZ4EOKhg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Spliting\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1)"
      ],
      "metadata": {
        "id": "ix_r6S8WLHN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why? "
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "80:20\n",
        "\n",
        "As it is most commonly used ratio."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**No the data set is balance.** "
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)\n"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1\n"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing necessary modules for Deploying models and Evaluating them\n",
        "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error, max_error\n",
        "from sklearn.linear_model import  LinearRegression, Lasso, Ridge\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.svm import SVR, LinearSVR\n",
        "from sklearn.neighbors import KNeighborsRegressor                                                                                                                                                                                                                                                #mahinisawesomemate\n",
        "from sklearn.ensemble import AdaBoostRegressor, BaggingRegressor, GradientBoostingRegressor, RandomForestRegressor, VotingRegressor, StackingRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler"
      ],
      "metadata": {
        "id": "9YcS6FxK6678"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluation"
      ],
      "metadata": {
        "id": "ffLJ7mlSHgMJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install shap "
      ],
      "metadata": {
        "id": "bZfoLeKhHtue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shap"
      ],
      "metadata": {
        "id": "kGkkLCiKHfNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Dictionary that will store metrics for evaluated models\n",
        "#This will be converted DataFrame using display report function\n",
        "report = {\n",
        "    'model_type':[],\n",
        "    'model_name':[],                                                                                                                                                                                                                                                #mahinisawesomemate\n",
        "    'rmse':[],\n",
        "    'mae':[],\n",
        "    'R2':[],\n",
        "    'adjusted R2':[]\n",
        "\n",
        "}\n"
      ],
      "metadata": {
        "id": "-rKiynmIH7IU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to evaluate and update model and score\n",
        "def evaluate(modeltype, modelname, Model, X_train, y_train, X_test, y_test):\n",
        "  from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error, max_error\n",
        "  #making sure the same model is not re-entered again\n",
        "  if modelname in report['model_name']:                                                                                                                                                                                                                                                #mahinisawesomemate\n",
        "    print(\"Prexisting Model\")\n",
        "    return 0\n",
        "  #making a copy to prevent accidental data changes\n",
        "  X_tr = X_train.copy()\n",
        "  X_te = X_test.copy()\n",
        "\n",
        "  #Fitting Model\n",
        "  Model.fit(X_tr, y_train)\n",
        "                                                                                                                                                                                                                                                #mahinisawesomemate\n",
        "  #Predicting Values from test set using model\n",
        "  y_pred = Model.predict(X_te)\n",
        "\n",
        "  #Model Evaluation\n",
        "\n",
        "  #Mean Absolute Error\n",
        "  mae = mean_absolute_error(y_test,y_pred)\n",
        "  report['mae'].append(mae) #Appending Metric\n",
        "\n",
        "  #R2 score\n",
        "  R2 = r2_score(y_test,y_pred)\n",
        "  report['R2'].append(R2) #Appending Metric\n",
        "\n",
        "  #Root Mean Square Error\n",
        "  rmse = np.sqrt(mean_squared_error(y_test,y_pred))\n",
        "  report['rmse'].append(rmse) #Appending Metric\n",
        "  \n",
        "  #Adjusted R2 score\n",
        "  adj_r2=1-(1-R2)*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1))\n",
        "  report['adjusted R2'].append(adj_r2) #Appending Metric\n",
        "\n",
        "  #Appending Model Details\n",
        "  report['model_name'].append(modelname)\n",
        "  report['model_type'].append(modeltype)\n",
        "  \n",
        "  print(f\"\\n\\n\\n----------\\n\")\n",
        "  \n",
        "  #Plotting Graph of observed vs predicted values\n",
        "  plt.figure(figsize=(20,10))\n",
        "  plt.plot((y_pred)[:100])\n",
        "  plt.plot((np.array(y_test)[:100]))\n",
        "  plt.legend([\"Predicted\",\"Actual\"])\n",
        "  plt.title(modelname)\n",
        "  plt.show()\n",
        "\n",
        "  print(f\"\\n\\n\\n----------\\n\")"
      ],
      "metadata": {
        "id": "TvUchBasIYil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#displays report in a dataframe\n",
        "def display_report():\n",
        "  return pd.DataFrame(report) "
      ],
      "metadata": {
        "id": "oz8033OYIfki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Getting Models "
      ],
      "metadata": {
        "id": "cRqHOJshIjXt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#inatlling catboost\n",
        "!pip install catboost"
      ],
      "metadata": {
        "id": "2RzL__tzIpqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from catboost import CatBoostRegressor\n",
        "import lightgbm as lgb\n",
        "from xgboost import XGBRegressor"
      ],
      "metadata": {
        "id": "PR2OGtYLJCeU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function that returns 3 arrays of model-functions, names and their details \n",
        "def get_models():\n",
        "  models, names, model_type = list(), list(), list()\n",
        "\n",
        "  # LinearReg\n",
        "  models.append(LinearRegression())\n",
        "  names.append('Linear Regression')\n",
        "  model_type.append('Linear')\n",
        "\n",
        "  #Lasso\n",
        "  models.append(Lasso(alpha =0.2))\n",
        "  names.append('Lasso Regression')\n",
        "  model_type.append('Regularized Linear (Lasso) ')\n",
        "\n",
        "  #Ridge\n",
        "  models.append(Ridge(alpha =0.5))\n",
        "  names.append('Ridge Regression')\n",
        "  model_type.append('Regularized Linear (Ridge)')\n",
        "  \n",
        "  # DecisionTree\n",
        "  models.append((DecisionTreeRegressor()))\n",
        "  names.append('DecisionTree Regressor')\n",
        "  model_type.append('CART')\n",
        "\n",
        "  #RandomForest\n",
        "  models.append(RandomForestRegressor())\n",
        "  names.append('RandomForest Regressor')\n",
        "  model_type.append('Ensemble Method')\n",
        "\n",
        "  # GradientBoosting\n",
        "  models.append(GradientBoostingRegressor())\n",
        "  names.append('GradientBoosting Regressor')\n",
        "  model_type.append('Ensemble Method')\n",
        "\n",
        "  # CatBoosting\n",
        "  models.append(CatBoostRegressor(silent = True))\n",
        "  names.append('Cat Boosting Regressor')\n",
        "  model_type.append('Ensemble Method')\n",
        "\n",
        "  \n",
        "  #Bagging\n",
        "  models.append(BaggingRegressor())\n",
        "  names.append('Bagging Regressor')\n",
        "  model_type.append('Ensemble Method')\n",
        "\n",
        "  #LightGBM Regressor\n",
        "  models.append(lgb.LGBMRegressor())\n",
        "  names.append('LightGBM Regressor')\n",
        "  model_type.append('Ensemble Method')\n",
        "  \n",
        "  # KNN\n",
        "  models.append(KNeighborsRegressor())\n",
        "  names.append('K Neighbors Regressor')\n",
        "  model_type.append('Neighbours')\n",
        "\n",
        "  \n",
        "\n",
        "  \n",
        "  return models, names, model_type"
      ],
      "metadata": {
        "id": "ymioeEhSJM22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Deployment"
      ],
      "metadata": {
        "id": "djXlvIqkJfhy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#scaling\n",
        "X_train_scaled, X_test_scaled = do_scale(X_train,X_test)"
      ],
      "metadata": {
        "id": "vo1NMWewJdFM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_scaled.shape, X_test_scaled.shape"
      ],
      "metadata": {
        "id": "Su6mnZcXJrwh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "#evaluating all modules in models and printing prediction graph\n",
        "Model, modelname, modeltype = get_models() \n",
        "for i in range(len(Model)):\n",
        "  evaluate(modeltype[i], modelname[i], Model[i], X_train_scaled , y_train, X_test_scaled,y_test)"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_report().sort_values('R2', ascending=False)"
      ],
      "metadata": {
        "id": "9hpB025FKaHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Cat Boosting Regressor,LightGBM and Random Forest Regressor have emerged to be the best performing models with R2 scores of 0.922, 0.917 and 0.914**"
      ],
      "metadata": {
        "id": "-agoTwuuPjSV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Creating Function to train linear models and calculate scores"
      ],
      "metadata": {
        "id": "tUiKuCgPsepi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV "
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cbc = CatBoostRegressor(silent = True)\n",
        "\n",
        "#create the grid\n",
        "grid = {'max_depth': [1,3,5,10,12],\n",
        "        'n_estimators':[100,200,350,400]\n",
        "        }\n",
        "\n",
        "#Instantiate GridSearchCV\n",
        "gscv = GridSearchCV (estimator = cbc, param_grid = grid, scoring ='r2', cv = 3)\n",
        "\n",
        "#fit the model\n",
        "gscv.fit(X_train_scaled,y_train)\n",
        "print(gscv.best_estimator_)\n"
      ],
      "metadata": {
        "id": "BvV5L2p2P1uK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#returns the best score\n",
        "print(gscv.best_score_)                                                                                                                                                                                                                                                #mahinisawesomemate\n",
        "#returns the best parameters\n",
        "print(gscv.best_params_)"
      ],
      "metadata": {
        "id": "SgDJPthpDXqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluation\n",
        "tuned_catboost = CatBoostRegressor(**gscv.best_params_, silent=True)\n",
        "evaluate(\"Ensemble Method\", \"TUNED CatBoost Model\", tuned_catboost, X_train_scaled , y_train, X_test_scaled,y_test)"
      ],
      "metadata": {
        "id": "gvbtOSRrFdq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_report().tail(1)"
      ],
      "metadata": {
        "id": "Kf1jpmf4HtEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **After tuning the parameters, CatBoosting's R2 scored has increased from 0.9235 to 0.9368**"
      ],
      "metadata": {
        "id": "_ouRX5znCEbT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We have used gridsearchCV method because it is a method for locating the ideal parameter values in a grid from a set of parameters is called GridSearchCV. In essence, it is a cross-validation method. Both the parameters and the model need to be entered. Predictions are performed after extracting the ideal parameter values.**"
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **After tuning the parameters, CatBoosting's R2 scored has increased from 0.9235 to 0.9368**"
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SHAP"
      ],
      "metadata": {
        "id": "X9SayAJ4IDkh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "explainer = shap.TreeExplainer(gscv.best_estimator_)\n",
        "shap_values = explainer.shap_values(X_test_scaled)"
      ],
      "metadata": {
        "id": "FkXWJB5UICCC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shap.initjs()\n",
        "i = 5\n",
        "shap.force_plot(explainer.expected_value, shap_values[i], features=X_test_scaled.iloc[i], feature_names=X_train_scaled.columns)"
      ],
      "metadata": {
        "id": "a-rxiV1YKNBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shap.summary_plot(shap_values, features= X_test_scaled, feature_names= X_test_scaled.columns, plot_type='bar')"
      ],
      "metadata": {
        "id": "AUmX3yuBWqep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "random.seed(23)\n",
        "index = (random.randint(0,len(X_test_scaled)))"
      ],
      "metadata": {
        "id": "nHfyZ4-LW3R7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Lime Explainability"
      ],
      "metadata": {
        "id": "wGSTo71pW9Yu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install lime "
      ],
      "metadata": {
        "id": "iqVpKObnW53K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import lime\n",
        "from lime.lime_tabular import LimeTabularExplainer"
      ],
      "metadata": {
        "id": "SoEn4SNRXeLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_names =list(X_train_scaled.columns)\n",
        "\n",
        "explainer = LimeTabularExplainer(np.array(X_train_scaled),\n",
        "    feature_names=feature_names,\n",
        "    mode = 'regression')              "
      ],
      "metadata": {
        "id": "QKV3GVAdXnoX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "exp = explainer.explain_instance(\n",
        "      data_row=X_test_scaled.iloc[index], \n",
        "      predict_fn= gscv.best_estimator_.predict\n",
        ")\n",
        "print(f\"-----------\\nACTUAL VALUE OBSERVED: {y_test.iloc[index]}\\n\")\n",
        "exp.show_in_notebook(show_table=True)"
      ],
      "metadata": {
        "id": "jX_baPUYXvB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model\n",
        "rfr = RandomForestRegressor()\n",
        "\n",
        "#create the grid\n",
        "grid = {'max_depth': [24,25,26],\n",
        "        'n_estimators':[375,400,425],\n",
        "        'max_samples':[0.5,0.6,0.8],\n",
        "        'max_features':[10,15,20]\n",
        "        }\n",
        "\n",
        "#Instantiate GridSearchCV\n",
        "gscv2 = GridSearchCV (estimator = rfr, param_grid = grid, scoring ='r2', cv = 3)                                                                                                                                                                                                                                                #mahinisawesomemate\n",
        "  \n",
        "\n",
        "#fit the model\n",
        "gscv2.fit(X_train_scaled,y_train)                                                                                                                                                                                                                                                #mahinisawesomemate\n",
        "  \n",
        "print(gscv2.best_estimator_)"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#returns the best score\n",
        "print(gscv2.best_score_)                                                                                                                                                                                                                                                #mahinisawesomemate\n",
        "#returns the best parameters\n",
        "print(gscv2.best_params_)"
      ],
      "metadata": {
        "id": "uIR_YDrGf3i4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate(\"Ensemble Method\", \"TUNED Random Forest Model\", gscv2.best_estimator_, X_train_scaled , y_train, X_test_scaled,y_test)"
      ],
      "metadata": {
        "id": "8-JNV0NjgXxS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_report().tail(1) "
      ],
      "metadata": {
        "id": "K6pVTtwygeSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Upon Hyperparameter testing, the R2 score for Random Forest has increased from .0.896 to 0.903**"
      ],
      "metadata": {
        "id": "rD0OS3GZgm7n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SHAP"
      ],
      "metadata": {
        "id": "znktPsewguvq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "explainer = shap.TreeExplainer(gscv2.best_estimator_)\n",
        "shap_values = explainer.shap_values(X_test_scaled[:200])"
      ],
      "metadata": {
        "id": "39dWo50Yg1-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shap.initjs()\n",
        "i = 5\n",
        "shap.force_plot(explainer.expected_value, shap_values[i], features=X_test_scaled.iloc[i], feature_names=X_train_scaled.columns)"
      ],
      "metadata": {
        "id": "DfQ1IHKFg6hX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shap.summary_plot(shap_values, features= X_test_scaled, feature_names= X_test_scaled.columns, plot_type='bar') "
      ],
      "metadata": {
        "id": "ttoOFMiag6d0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#LIME Explainability"
      ],
      "metadata": {
        "id": "4LZRwxr-iHRJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_names =list(X_train_scaled.columns)\n",
        "\n",
        "explainer = LimeTabularExplainer(np.array(X_train_scaled),\n",
        "    feature_names=feature_names,\n",
        "    mode = 'regression')  "
      ],
      "metadata": {
        "id": "Ty23kT5wiEox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "exp = explainer.explain_instance(\n",
        "      data_row=X_test_scaled.iloc[index], \n",
        "      predict_fn= gscv2.best_estimator_.predict\n",
        ")\n",
        "print(f\"-----------\\nACTUAL VALUE OBSERVED: {y_test.iloc[index]}\\n\")\n",
        "exp.show_in_notebook(show_table=True)"
      ],
      "metadata": {
        "id": "j5tptzTFiXJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We have used gridsearchCV method because it is a method for locating the ideal parameter values in a grid from a set of parameters is called GridSearchCV. In essence, it is a cross-validation method. Both the parameters and the model need to be entered. Predictions are performed after extracting the ideal parameter values.**"
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **The tuned LightGBM regressor has produced a better performance than the tuned LightGBM regressor, from 0.909 to 0.921**"
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model\n",
        "\n",
        "params = {\n",
        "\n",
        "        \n",
        "        'max_depth': (19,20,25),\n",
        "        'n_estimators':[375,400,425],\n",
        "        'max_features':[10,15,20]\n",
        "\n",
        "\n",
        "}\n",
        "\n",
        "# Initialize a GridSearchCV -\n",
        "gscv3 = GridSearchCV(estimator=lgb.LGBMRegressor(), param_grid =  params, cv =  3,verbose=1)                                                                                                                                                                                                                                                #mahinisawesomemate\n",
        "  \n",
        "\n",
        "# Train on training data-\n",
        "gscv3.fit(X_train_scaled, y_train,verbose=1)"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model\n",
        "\n",
        "params = {\n",
        "\n",
        "        \n",
        "        'max_depth': (19,20,25),\n",
        "        'n_estimators':[375,400,425],\n",
        "        'max_features':[10,15,20]\n",
        "\n",
        "\n",
        "}\n",
        "\n",
        "# Initialize a GridSearchCV -\n",
        "gscv3 = GridSearchCV(estimator=lgb.LGBMRegressor(), param_grid =  params, cv =  3,verbose=1)                                                                                                                                                                                                                                                #mahinisawesomemate\n",
        "  \n",
        "\n",
        "# Train on training data-\n",
        "gscv3.fit(X_train_scaled, y_train,verbose=1)"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#returns the best score\n",
        "print(gscv3.best_score_)                                                                                                                                                                                                                                                #mahinisawesomemate\n",
        "#returns the best parameters\n",
        "print(gscv3.best_params_)"
      ],
      "metadata": {
        "id": "78vqX34pjTqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluate\n",
        "evaluate(\"Ensemble Method\", \"TUNED LGBM\", gscv3.best_estimator_, X_train_scaled , y_train, X_test_scaled,y_test)"
      ],
      "metadata": {
        "id": "mmCXHFwUjhGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_report().tail(1)"
      ],
      "metadata": {
        "id": "N5r3TE0AjuJC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **The tuned LightGBM regressor has produced a better performance than the tuned LightGBM regressor, from 0.909 to 0.921**"
      ],
      "metadata": {
        "id": "P-Un0P9pj0A8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SHAP"
      ],
      "metadata": {
        "id": "-R7Wazj_n-C6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "explainer = shap.TreeExplainer(gscv3.best_estimator_)\n",
        "shap_values = explainer.shap_values(X_test_scaled)"
      ],
      "metadata": {
        "id": "sSdlE7zIjt5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shap.initjs()\n",
        "i = 5\n",
        "shap.force_plot(explainer.expected_value, shap_values[i], features=X_test_scaled.iloc[i], feature_names=X_train_scaled.columns)"
      ],
      "metadata": {
        "id": "X9DU-TODoEQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shap.summary_plot(shap_values, features= X_test_scaled, feature_names= X_test_scaled.columns, plot_type='bar')"
      ],
      "metadata": {
        "id": "1661Uc7DoSBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**LIME Explainability**"
      ],
      "metadata": {
        "id": "c79DXsP9ojFO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_names =list(X_train_scaled.columns)\n",
        "\n",
        "explainer = LimeTabularExplainer(np.array(X_train_scaled),\n",
        "    feature_names=feature_names,\n",
        "    mode = 'regression') "
      ],
      "metadata": {
        "id": "I7JG1XKHozrE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "exp = explainer.explain_instance(\n",
        "      data_row=X_test_scaled.iloc[index], \n",
        "      predict_fn= gscv3.best_estimator_.predict\n",
        ")\n",
        "print(f\"-----------\\nACTUAL VALUE OBSERVED: {y_test.iloc[index]}\\n\")\n",
        "exp.show_in_notebook(show_table=True)"
      ],
      "metadata": {
        "id": "zilMemgTpVvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used gridsearchCV . Because it gives more eeficient result."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **The tuned LightGBM regressor has produced a better performance than the tuned LightGBM regressor, from 0.909 to 0.921**"
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We are going to perform stacking for that**"
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Stacking"
      ],
      "metadata": {
        "id": "60dFpusKDRBA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alternatively, we are going to try and stack the top 3 best performing regressors and evaluate its performance against the best performing model"
      ],
      "metadata": {
        "id": "6U0cB7DNDY_J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "level0 = list()\n",
        "level0.append(('LGBM', gscv3.best_estimator_)) #Light GBM                                                                                                                                                                                                                                                #mahinisawesomemate\n",
        "level0.append(('Random Forest', gscv2.best_estimator_)) #Tuned Random Forest Model\n",
        "level0.append(('Cat Boost', gscv.best_estimator_)) #Tuned CatBoost\n",
        "# define meta learner model\n",
        "level1 = LinearRegression()"
      ],
      "metadata": {
        "id": "XdQKPRc2DVi7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stacking_model = StackingRegressor(estimators=level0, final_estimator=level1, cv=5, passthrough = True)"
      ],
      "metadata": {
        "id": "GkXgqTtzDiMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate(\"Ensemble Method\", \"STACKING Regressor\", stacking_model, X_train_scaled , y_train, X_test_scaled,y_test)"
      ],
      "metadata": {
        "id": "CL41WNAzDwU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_report().tail(1)"
      ],
      "metadata": {
        "id": "lA-th6QvPqbB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> * **Stacking all three top learners gives an R2 score of 0.9379**\n",
        ">* **The Stacked model has better R2 scores than the individual learners**"
      ],
      "metadata": {
        "id": "MFDWme8yRSZU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#LIME EXPLAINABILITY"
      ],
      "metadata": {
        "id": "q9tJGMaORc1H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stacking_model.fit(X_train_scaled , y_train)\n",
        "\n",
        "feature_names =list(X_train_scaled.columns)\n",
        "\n",
        "explainer = LimeTabularExplainer(np.array(X_train_scaled),\n",
        "    feature_names=feature_names,\n",
        "    mode = 'regression') "
      ],
      "metadata": {
        "id": "IXul_CsFRafn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "exp = explainer.explain_instance(\n",
        "      data_row=X_test_scaled.iloc[index], \n",
        "      predict_fn= stacking_model.predict\n",
        ")\n",
        "print(f\"-----------\\nACTUAL VALUE OBSERVED: {y_test.iloc[index]}\\n\")\n",
        "exp.show_in_notebook(show_table=True)"
      ],
      "metadata": {
        "id": "gGzHG8pCRpgw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Performance Report "
      ],
      "metadata": {
        "id": "Ve6thnULTAtf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display_report().sort_values('R2',ascending= False)"
      ],
      "metadata": {
        "id": "_jvqnk3YTLi-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **The top performing models based on R2 scores are Stacking Regressor and Parameter tuned CatBoosting Regressor.**"
      ],
      "metadata": {
        "id": "mFszHPWqT7b_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display_report().to_csv('Final_Report.csv')"
      ],
      "metadata": {
        "id": "Gh06ZFkwXmyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating bar plot to visualize the  R2 of model \n",
        "plt.figure(figsize=(14,6),dpi=80)\n",
        "sns.barplot(x='model_name',y='R2',data=display_report().sort_values('R2'),palette=\"flare\").set(title='R2 of model_name')\n",
        "plt.xticks(rotation=90);"
      ],
      "metadata": {
        "id": "9VFrKmWZVqAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**I have used LIGHT GBM Model because LightGBM has many of XGBoost's advantages, including sparse optimization, parallel training, multiple loss functions, regularization, bagging, and early stopping.**"
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6FuAPXyfUCZh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data."
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write the conclusion here."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}